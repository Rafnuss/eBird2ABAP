{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from functions.latlng2pentad import latlng2pentad\n",
    "from functions.pentad2latlng import pentad2latlng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A card is defined by three elements: pentad_code, user_id, and date of the first day of the 5 days.\n",
    "\n",
    "From a card id {pentad}_{observer}_{date}, it is possible to find all checklists that belong to it.\n",
    "\n",
    "The aim here is to build a list of valid cards which will then be used to find the checklist_id which belongs to this card and then finally compute the card info from the list of checklists.\n",
    "\n",
    "Pentad: we need to assign for each checklist its pentad and check that the distance traveled is within the boundary of the pentad.\n",
    "User_id is quite straightforward to build.\n",
    "Date: much more challenging. See below for details.\n",
    "\n",
    "## Set up the Import Options and import the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = \"KE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd0 = pd.read_csv(f\"data/eBird/ebd_{cntr}_relAug-2022/ebd_{cntr}_relAug-2022.txt\", \n",
    "                   delimiter=\"\\t\",\n",
    "                   usecols=[\"SAMPLING EVENT IDENTIFIER\", \"SCIENTIFIC NAME\", \"CATEGORY\", \"LATITUDE\", \"LONGITUDE\", \"OBSERVATION DATE\", \"TIME OBSERVATIONS STARTED\", \"PROTOCOL TYPE\", \"DURATION MINUTES\", \"EFFORT DISTANCE KM\", \"ALL SPECIES REPORTED\", \"OBSERVER ID\"],\n",
    "                   parse_dates=[\"OBSERVATION DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OBSERVATIONDATETIME by combining date and time\n",
    "tmp = ebd0['TIME OBSERVATIONS STARTED'].fillna(\"00:00:00\")\n",
    "ebd0['OBSERVATION DATETIME'] = pd.to_datetime(ebd0['OBSERVATION DATE'].dt.strftime('%Y-%m-%d') + \" \" + tmp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort by date: Important to have for filtering duplicate card-adu and needed for sequence\n",
    "ebd0.sort_values(by=\"OBSERVATION DATE\", inplace=True)\n",
    "\n",
    "# Keep only species category\n",
    "# ebd0[[\"COMMONNAME\", \"SCIENTIFIC NAME\", \"CATEGORY\"]].drop_duplicates().to_csv(\"species_list_ebird.csv\", index=False)\n",
    "\n",
    "# Keep some spuh which can be matched to an ADU\n",
    "# spuh_keep = pd.read_csv(\"data/spuh_keep.csv\", dtype=str)\n",
    "# ebd0 = ebd0[(~ebd0[\"CATEGORY\"].isin([\"spuh\", \"slash\"])) | ebd0[\"SCIENTIFIC NAME\"].isin(spuh_keep[\"Clements--scientific_name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SCIENTIFIC NAME\n",
       "Phyllastrephus sp.                1\n",
       "Ardea sp.                         1\n",
       "Ardeidae sp.                      1\n",
       "Ardenna grisea/tenuirostris       1\n",
       "Otididae sp.                      1\n",
       "                               ... \n",
       "Ortygornis sephaena            5450\n",
       "Eurocephalus ruppelli          5737\n",
       "Microcarbo africanus           7540\n",
       "Buphagus erythrorynchus        7908\n",
       "Crithagra striolata            8122\n",
       "Name: SCIENTIFIC NAME, Length: 262, dtype: int64"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read species_match data\n",
    "species_match = pd.read_excel(\"data/World list - working v1.xlsx\")\n",
    "species_match.rename(columns={\"Sci (eBird)\": \"SCIENTIFIC NAME\"}, inplace=True)\n",
    "species_match = species_match[~species_match[\"ADU\"].isna() & ~species_match[\"SCIENTIFIC NAME\"].isna()][[\"ADU\", \"SCIENTIFIC NAME\"]]\n",
    "\n",
    "# Check that all entries are matching\n",
    "matching_entries = ebd0[~ebd0[\"SCIENTIFIC NAME\"].isin(species_match[\"SCIENTIFIC NAME\"])]\n",
    "matching_entries.groupby(\"SCIENTIFIC NAME\")[\"SCIENTIFIC NAME\"].count().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build checklist level dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ebd = ebd0.copy()\n",
    "ebd.drop_duplicates([\"SAMPLING EVENT IDENTIFIER\", \"LATITUDE\", \"LONGITUDE\", \"OBSERVATION DATE\", \"OBSERVATION DATETIME\", \"PROTOCOL TYPE\", \"DURATION MINUTES\", \"EFFORT DISTANCE KM\", \"ALL SPECIES REPORTED\", \"OBSERVER ID\"], inplace=True)\n",
    "ebd.sort_values(by=\"OBSERVATION DATE\", inplace=True)\n",
    "\n",
    "# Filter protocol\n",
    "ebd[\"KEEP PROTOCOL\"] = ebd[\"PROTOCOL TYPE\"].isin([\"Historical\", \"Incidental\", \"Stationary\", \"Traveling\"])\n",
    "\n",
    "# Pentad\n",
    "# Assign to pentad and check if distance remains inside\n",
    "ebd[\"PENTAD\"] = latlng2pentad(ebd[\"LATITUDE\"], ebd[\"LONGITUDE\"])\n",
    "\n",
    "# Search center of pentad\n",
    "lat, lon = pentad2latlng(ebd[\"PENTAD\"])\n",
    "dist = (5 / 60 / 2) * 1.2  # allow for a 20% overlap\n",
    "\n",
    "effort_distance_deg = (180/np.pi) *  ebd[\"EFFORT DISTANCE KM\"] / 6371\n",
    "ebd[\"KEEP PENTAD\"] = ~((effort_distance_deg + np.maximum(np.abs(lat - ebd[\"LATITUDE\"]), np.abs(lon - ebd[\"LONGITUDE\"]))) > dist)\n",
    "\n",
    "# Also filter historical checklists which have no distance\n",
    "ebd.loc[(ebd[\"PROTOCOL TYPE\"] == \"Historical\") & ebd[\"EFFORT DISTANCE KM\"].isna(), \"KEEP PENTAD\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find valid full protocol card\n",
    "\n",
    "Cards are considered to be full protocol if the sum of durations of the underlying checklists exceed 2 hours over the next rolling 5 days.\n",
    "In this section, we first indentify which checklists can create a valid full card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of all checklists which contribute to the 2hr rule. Note that we will still use \"non-valid\" checklists later as their species still contribute to the card.\n",
    "valid_id = (ebd[\"KEEP PENTAD\"] & ebd[\"KEEP PROTOCOL\"] & (ebd[\"DURATION MINUTES\"] > 0) & ebd[\"ALL SPECIES REPORTED\"])\n",
    "\n",
    "# Filter for valid checklist and create in a smaller table\n",
    "check = ebd.loc[valid_id, [\"PENTAD\", \"OBSERVER ID\", \"OBSERVATION DATE\", \"DURATION MINUTES\"]]\n",
    "\n",
    "# Combine checklists made by the same observer, pentad, and day. This is an intermediate step which enables us to grid the 5days windows more easily\n",
    "checkday = check.groupby([\"PENTAD\", \"OBSERVER ID\", \"OBSERVATION DATE\"]).agg({\"DURATION MINUTES\": \"sum\"}).reset_index()\n",
    "\n",
    "# Sort the checklist by id and date\n",
    "checkday.sort_values(by=[\"OBSERVATION DATE\"], inplace=True)\n",
    "\n",
    "# Create additional columns\n",
    "checkday[\"pentad_observer\"] = checkday[\"PENTAD\"] + \"_\" + checkday[\"OBSERVER ID\"]\n",
    "checkday[\"pentad_observer_date\"] = checkday[\"PENTAD\"] + \"_\" + checkday[\"OBSERVER ID\"].str[3:] + \"_\" + checkday[\"OBSERVATION DATE\"].dt.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize card column. We will only assign a card to checklists contributing to a full protcol card.\n",
    "checkday[\"CARD\"] = \"\"\n",
    "\n",
    "# Loop through each pentad_observer\n",
    "for po in checkday[\"pentad_observer\"].unique():\n",
    "    pentad_observer = checkday[checkday[\"pentad_observer\"] == po].index\n",
    "    di = np.abs(checkday.loc[pentad_observer, \"OBSERVATION DATE\"].values[:, None] - checkday.loc[pentad_observer, \"OBSERVATION DATE\"].values) < pd.Timedelta(days=5)\n",
    "    u = 1\n",
    "    while u <= len(pentad_observer):\n",
    "        nb_neighbor = np.sum(di[u-1, (u-1):])\n",
    "        neigh = u + np.arange(0, nb_neighbor)\n",
    "        dur = checkday.loc[pentad_observer[neigh-1], \"DURATION MINUTES\"].sum()\n",
    "        if dur >= (2*60):\n",
    "            checkday.loc[pentad_observer[neigh-1], \"CARD\"] = checkday.loc[pentad_observer[u-1], \"pentad_observer_date\"]\n",
    "        u += nb_neighbor\n",
    "\n",
    "# Create the card DataFrame\n",
    "card = checkday[checkday[\"CARD\"] == checkday[\"pentad_observer_date\"]][[\"PENTAD\", \"OBSERVER ID\", \"OBSERVATION DATE\", \"CARD\"]]\n",
    "\n",
    "# Sort by card\n",
    "card.sort_values(by='CARD', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Card\n",
    "\n",
    "We take back `ebd` where all checklists (i.e., including the incidentals, stationary, etc...) and find if they contribute to an existing full card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'card' column in 'ebd' DataFrame\n",
    "ebd['CARD'] = \"\"\n",
    "\n",
    "# Initialize a list of dictionaries to store the converted data\n",
    "d = []# [{} for _ in range(len(card))]\n",
    "\n",
    "for index, row in card.iterrows():\n",
    "    id = (ebd['KEEP PENTAD'] &\n",
    "          (ebd['PENTAD'] == row['PENTAD']) &\n",
    "          (ebd['OBSERVER ID'] == row['OBSERVER ID']) &\n",
    "          (ebd['OBSERVATION DATE'] >= row['OBSERVATION DATE']) &\n",
    "          (ebd['OBSERVATION DATE'] < row['OBSERVATION DATE'] + pd.Timedelta(5, unit=\"D\")))\n",
    "\n",
    "    assert all(ebd.loc[id, 'CARD'] == \"\")\n",
    "    ebd.loc[id, 'CARD'] = row['CARD']\n",
    "\n",
    "    # Create a dictionary for each card\n",
    "    d.append({\n",
    "        'Protocol': 'F',\n",
    "        'ObserverEmail': 'kenyabirdmap@naturekenya.org',\n",
    "        'CardNo': row['CARD'],\n",
    "        'StartDate': str(ebd.loc[id, 'OBSERVATION DATE'].min().date()),\n",
    "        'EndDate': str(ebd.loc[id, 'OBSERVATION DATE'].max().date()),\n",
    "        'StartTime': str(ebd.loc[id, 'OBSERVATION DATETIME'].min().time()),\n",
    "        'Pentad': row['PENTAD'],\n",
    "        'ObserverNo': '22829',\n",
    "        'TotalHours': np.nansum(ebd.loc[id, 'DURATION MINUTES']) / 60,\n",
    "        'Hour1': \"\",\n",
    "        'Hour2': \"\",\n",
    "        'Hour3': \"\",\n",
    "        'Hour4': \"\",\n",
    "        'Hour5': \"\",\n",
    "        'Hour6': \"\",\n",
    "        'Hour7': \"\",\n",
    "        'Hour8': \"\",\n",
    "        'Hour9': \"\",\n",
    "        'Hour10': \"\",\n",
    "        'TotalSpp': 0,\n",
    "        'InclNight': \"0\",\n",
    "        'AllHabitats': \"0\",\n",
    "        'Checklists': ebd.loc[id, 'SAMPLING EVENT IDENTIFIER'].tolist(),\n",
    "        'TotalDistance': np.nansum(ebd.loc[id, 'EFFORT DISTANCE KM']),\n",
    "        'ObserverNoEbird': row['OBSERVER ID']\n",
    "    })\n",
    "\n",
    "# 'd' now contains the converted data for each card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get species level information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the full dataset to get only the checklist  valid...\n",
    "all_checklists = [item for x in d for item in x[\"Checklists\"]]\n",
    "ebd0f = ebd0.loc[ebd0[\"SAMPLING EVENT IDENTIFIER\"].isin(all_checklists),[\"SAMPLING EVENT IDENTIFIER\", \"SCIENTIFIC NAME\", \"OBSERVATION DATETIME\"]]\n",
    "\n",
    "# Add card No\n",
    "ebd0f = pd.merge(ebd0f, ebd.loc[:,[\"SAMPLING EVENT IDENTIFIER\", \"CARD\"]], left_on=\"SAMPLING EVENT IDENTIFIER\", right_on=\"SAMPLING EVENT IDENTIFIER\", how=\"left\")\n",
    "\n",
    "# Keep a unique list of card-species (remove duplicate species in the same card)\n",
    "ebd0f.sort_values(by=\"OBSERVATION DATETIME\", inplace=True) # SHould have been done already above, but necessary for keep=\"first\"\n",
    "ebd0fu = ebd0f.drop_duplicates(subset=[\"CARD\", \"SCIENTIFIC NAME\"], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ADU number\n",
    "ebd0fus = pd.merge(ebd0fu, species_match, how=\"left\")\n",
    "\n",
    "# Set un-macthed species to undefined\n",
    "ebd0fus['ADU'] = ebd0fus['ADU'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sequence of records based on datetime entry\n",
    "ebd0fus[\"SEQ\"] = ebd0fus.groupby(\"CARD\")[\"OBSERVATION DATETIME\"].rank(method=\"min\") # dense t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the species list per card as a cell for vectorized computation\n",
    "card_list = ebd0fus.groupby(\"CARD\")[[\"ADU\", \"SEQ\"]].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in card_list.iterrows():\n",
    "    \n",
    "    assert(c[\"CARD\"] == d[i][\"CardNo\"])\n",
    "    d[i][\"records\"] = []\n",
    "\n",
    "    for adu, seq in zip(c[\"ADU\"], c[\"SEQ\"]):\n",
    "        d[i][\"records\"].append({\n",
    "            \"Sequence\": seq,\n",
    "            \"Latitude\": \"\",\n",
    "            \"Longitude\": \"\",\n",
    "            \"Altitude\": \"\",\n",
    "            \"CardNo\": d[i][\"CardNo\"],\n",
    "            \"Spp\": adu,\n",
    "            \"Accuracy\": \"\",\n",
    "            \"SightingTime\": \"\"\n",
    "        })\n",
    "\n",
    "    d[i][\"TotalSpp\"] = len(c[\"ADU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"export/{cntr}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", 'w') as f:\n",
    "    json.dump(d, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
