{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to compute ABAP card from eBird data\n",
    "\n",
    "The aims of this code is to produce a dataset of ABAP full protocol card equivalent from the eBird EBD dataset.\n",
    "\n",
    "## Overview rational of the approach\n",
    "\n",
    "A card is uniquely defined by three elements: pentad*code, user_id, and date of the first day of the 5 days. We therefore use the `card_id = {pentad}*{observer}\\_{date}`. From these three variables, it is possible to find all checklists that belong to it.\n",
    "\n",
    "The aim here is to build a list of **valid** cards for which will then be used to find the `checklist_id` that belongs to each valid card and then finally compute the card info from the list of checklists.\n",
    "\n",
    "Pentad: we need to assign for each checklist its pentad and check that the distance traveled is within the boundary of the pentad.\n",
    "User_id is quite straightforward to build.\n",
    "Date: much more challenging. See below for details.\n",
    "\n",
    "General steps:\n",
    "\n",
    "1. Construct the list of valid cards\n",
    "   1. Group raw EBD data to checklist level information (merge shared checklist)\n",
    "   2. Filter checklists which could make a valid card\n",
    "      1. Keep only complete checklists\n",
    "      2. Keep only checklists with `Historical`, `Stationary`, `Traveling`, `Incidental` protocol\n",
    "      3. Keep only checklists within the pentad, that is,\n",
    "         1. Exclude checklists with `historical` protocol that don't have a distance.\n",
    "         2. Exclude checklists with distance greater than the distance from center of checklist to closest pentad limit (accept some overlap with a correction factor).\n",
    "      4. Keep only checklists with duration greated than 0.\n",
    "   3. Group checklists by date (named `checkday` later on)\n",
    "   4. Group checklists into pentad_observer group so that we only have to loop through the date to find valid card\n",
    "   5. Preliminary filter to eliminate all pentad_observer for which the sum over the entire period does not lead to 2h\n",
    "   6. For each remaining pentad_observer, apply the function `checkday_pentad_observer()`, which,\n",
    "      1. Compute the temporal distance between all checkday and check if they are within 5 days.\n",
    "      2. Loop through all checkday,\n",
    "         1. Compute the total duration of all checkdays within temporal distance\n",
    "            1. If valid, create the card_id and apply it to all checkday. Iterate to the first next checkday that was not within temporal distance\n",
    "            2. If invalid, iterate to the next checkday\n",
    "2. Create the card data\n",
    "   1. For each valid card, aggregate all checklists which are (1) within pentad, (2) same observer and (3) day within the 5 day period. This include more checklists than used to construct the list of valid cards\n",
    "3. Add species level information to cards\n",
    "   1. Add sequence information based on first occurance on checklist.\n",
    "4. Export in JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General set-up\n",
    "\n",
    "### Load library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import csv\n",
    "import importlib.resources as pkg_resources\n",
    "\n",
    "# Add the parent directory to the system path so the notebook can find the eBird2ABAP package\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# Now import the necessary functions from utils.py\n",
    "from eBird2ABAP.utils import latlng2pentad, pentad2latlng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the latest EBD AFRICA data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code doesn't work as you need to sign in into to get access to the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_EBD(year=None, month=None):\n",
    "    if (year is None) | (month is None):\n",
    "        # Calculate previous month and year\n",
    "        today = datetime.date.today()\n",
    "        last_month = today.replace(day=1) - datetime.timedelta(days=1)\n",
    "        if year is None:\n",
    "            year = last_month.strftime(\"%Y\")\n",
    "        if month is None:\n",
    "            month = last_month.strftime(\"%b\")\n",
    "\n",
    "    # Construct URL and filename\n",
    "    url = f\"https://download.ebird.org/ebd/prepackaged/ebd_AFR_rel{month}-{year}.tar\"\n",
    "    filename = os.path.basename(url)\n",
    "    filepath = os.path.join(\"../data/eBird/\", filename)\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        print(f\"Request data at {url}\")\n",
    "        f.write(requests.get(url).content)\n",
    "\n",
    "    with tarfile.open(filepath, \"r\") as tar:\n",
    "        tar.extractall(f\"../data/eBird/ebd_AFR_rel{month}-{year}/\")\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request data at https://download.ebird.org/ebd/prepackaged/ebd_AFR_relJan-2025.tar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/eBird/ebd_AFR_relJan-2025.tar'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_EBD(\"2025\", \"Jan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the EBD file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_EBD(file, nrows=None):\n",
    "    ebd0 = pd.read_csv(\n",
    "        file,\n",
    "        delimiter=\"\\t\",\n",
    "        usecols=[\n",
    "            \"SAMPLING EVENT IDENTIFIER\",\n",
    "            \"GROUP IDENTIFIER\",\n",
    "            \"SCIENTIFIC NAME\",\n",
    "            \"TAXON CONCEPT ID\",\n",
    "            \"CATEGORY\",\n",
    "            \"LATITUDE\",\n",
    "            \"LONGITUDE\",\n",
    "            \"OBSERVATION DATE\",\n",
    "            \"TIME OBSERVATIONS STARTED\",\n",
    "            \"PROTOCOL TYPE\",\n",
    "            \"DURATION MINUTES\",\n",
    "            \"EFFORT DISTANCE KM\",\n",
    "            \"ALL SPECIES REPORTED\",\n",
    "            \"OBSERVER ID\",\n",
    "        ],\n",
    "        parse_dates=[\"OBSERVATION DATE\"],\n",
    "        nrows=nrows,  # Use this to read only a smaller portion of the file to run faster and test the code\n",
    "    )\n",
    "\n",
    "    ebd = ebd0\n",
    "\n",
    "    # Combine shared checklist: Overwrite sampling event identifier by the first one submitted (lower SXXXXXX value)\n",
    "    ebd[\"SAMPLING EVENT IDENTIFIER\"] = (\n",
    "        ebd.groupby(\"GROUP IDENTIFIER\")[\"SAMPLING EVENT IDENTIFIER\"]\n",
    "        .transform(\"min\")\n",
    "        .where(ebd[\"GROUP IDENTIFIER\"].notna(), ebd[\"SAMPLING EVENT IDENTIFIER\"])\n",
    "    )\n",
    "    # Drop the GROUP IDENTIFIER column\n",
    "    ebd = ebd.drop(columns=\"GROUP IDENTIFIER\")\n",
    "\n",
    "    # Create OBSERVATIONDATETIME by combining date and time\n",
    "    tmp = ebd[\"TIME OBSERVATIONS STARTED\"].fillna(\"00:00:00\")\n",
    "    ebd[\"OBSERVATION DATETIME\"] = pd.to_datetime(\n",
    "        ebd[\"OBSERVATION DATE\"].dt.strftime(\"%Y-%m-%d\") + \" \" + tmp,\n",
    "        format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Sort by date: Important to have for filtering duplicate card-adu and needed for sequence\n",
    "    ebd.sort_values(by=\"OBSERVATION DATETIME\", inplace=True)\n",
    "\n",
    "    # Keep only species category\n",
    "    # ebd0[[\"COMMONNAME\", \"SCIENTIFIC NAME\", \"CATEGORY\"]].drop_duplicates().to_csv(\"species_list_ebird.csv\", index=False)\n",
    "\n",
    "    # Keep some spuh which can be matched to an ADU\n",
    "    # spuh_keep = pd.read_csv(\"data/spuh_keep.csv\", dtype=str)\n",
    "    # ebd0 = ebd0[(~ebd0[\"CATEGORY\"].isin([\"spuh\", \"slash\"])) | ebd0[\"SCIENTIFIC NAME\"].isin(spuh_keep[\"Clements--scientific_name\"])]\n",
    "\n",
    "    return ebd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2025\"\n",
    "month = \"Jan\"\n",
    "\n",
    "# file = \"../data/eBird/ebd_AFR_rel{month}-{year}/ebd_AFR_rel{month}-{year}.txt.gz\"\n",
    "file = f\"../data/eBird/ebd_AFR_rel{month}-{year}/ebd_AFR_rel{month}-{year}.txt.gz\"\n",
    "# file = \"../data/eBird/ebd_AFR_relJul-2024/ebd_AFR_relJul-2024.txt.gz\"\n",
    "\n",
    "\n",
    "# For country specific EBD file, you can use:\n",
    "# cntr = \"KE\"\n",
    "# file = \"../data/eBird/chk_{cntr}_relAug-2022/ebd_{cntr}_relAug-2022.txt\"\n",
    "\n",
    "# file = f\"../data/eBird/ebd_NG_relAug-2022/ebd_NG_relAug-2022.txt\"\n",
    "ebd0 = read_EBD(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd = ebd0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add species taxonomy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matched_species():\n",
    "    with pkg_resources.files(\"eBird2ABAP\").joinpath(\"matched_species.csv\") as file_path:\n",
    "        print(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_matched_species():\n",
    "    return pd.read_csv(\"../data/species_list/matched_species.csv\")\n",
    "\n",
    "\n",
    "def add_ADU(ebd, return_unmatched=False):\n",
    "    # Read matched_species data. See species_match.ipynb\n",
    "    matched_species = load_matched_species()\n",
    "\n",
    "    ebd = pd.merge(\n",
    "        ebd,  # .loc[:,['OBSERVER ID', 'PENTAD', \"SAMPLING EVENT IDENTIFIER\", \"OBSERVATION DATE\"]],\n",
    "        matched_species[[\"SCIENTIFIC NAME\", \"ADU\"]].drop_duplicates(\n",
    "            subset=\"SCIENTIFIC NAME\"\n",
    "        ),\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if return_unmatched:\n",
    "        unmatched = (\n",
    "            ebd[ebd[\"ADU\"].isna()][[\"SCIENTIFIC NAME\"]]\n",
    "            .value_counts()\n",
    "            .sort_values(ascending=0)\n",
    "        )\n",
    "        print(\n",
    "            f\"We still have {len(unmatched)} unmatched taxons, corresponding to {round(sum(unmatched)/len(ebd)*100)}% of our data\"\n",
    "        )\n",
    "        return unmatched\n",
    "    else:\n",
    "        ebd[\"ADU\"] = ebd[\"ADU\"].fillna(0).astype(int)\n",
    "        return ebd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We still have 906 unmatched taxons, corresponding to 6% of our data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SCIENTIFIC NAME           \n",
       "Pycnonotus barbatus           335497\n",
       "Egretta garzetta              104411\n",
       "Lanius collaris                76514\n",
       "Corythornis cristatus          58702\n",
       "Ortygornis sephaena            48954\n",
       "                               ...  \n",
       "Caryothraustes poliogaster         1\n",
       "Cathartidae sp.                    1\n",
       "Centropus sinensis                 1\n",
       "Ceratogymna elata/atrata           1\n",
       "Ixos mcclellandii                  1\n",
       "Name: count, Length: 906, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched = add_ADU(ebd, return_unmatched=True)\n",
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd = add_ADU(ebd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# data: 23474053\n",
      "# Checklists: 1107447\n",
      "# Species: 3214\n"
     ]
    }
   ],
   "source": [
    "print(f\"# data: {len(ebd)}\")\n",
    "print(f\"# Checklists: {len(ebd['SAMPLING EVENT IDENTIFIER'].unique())}\")\n",
    "print(f\"# Species: {len(ebd['SCIENTIFIC NAME'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build checklist dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebd2chk(ebd):\n",
    "    chk = ebd[\n",
    "        [\n",
    "            \"SAMPLING EVENT IDENTIFIER\",\n",
    "            \"LATITUDE\",\n",
    "            \"LONGITUDE\",\n",
    "            \"OBSERVATION DATE\",\n",
    "            \"OBSERVATION DATETIME\",\n",
    "            \"PROTOCOL TYPE\",\n",
    "            \"DURATION MINUTES\",\n",
    "            \"EFFORT DISTANCE KM\",\n",
    "            \"ALL SPECIES REPORTED\",\n",
    "            \"OBSERVER ID\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Sort by date\n",
    "    chk.sort_values(by=\"OBSERVATION DATETIME\", inplace=True)\n",
    "\n",
    "    # For some shared checklist some variable are different for the same sampling event.\n",
    "    # chk[chk[\"SAMPLING EVENT IDENTIFIER\"].duplicated(keep=False)].sort_values(by=\"SAMPLING EVENT IDENTIFIER\")\n",
    "    # ebd0[ebd0[\"SAMPLING EVENT IDENTIFIER\"] == \"S97700871\"]\n",
    "    chk = chk.drop_duplicates(\"SAMPLING EVENT IDENTIFIER\").reset_index(drop=True)\n",
    "    len(chk)\n",
    "\n",
    "    # Filter protocol\n",
    "    chk[\"KEEP PROTOCOL\"] = chk[\"PROTOCOL TYPE\"].isin(\n",
    "        [\"Historical\", \"Incidental\", \"Stationary\", \"Traveling\"]\n",
    "    )\n",
    "\n",
    "    # Pentad\n",
    "    # Assign the pentad to all checklists based on their location\n",
    "    chk[\"PENTAD\"] = latlng2pentad(chk[\"LATITUDE\"], chk[\"LONGITUDE\"])\n",
    "\n",
    "    # Retrieve the lat, lon center of the assigned pentad\n",
    "    lat, lon = pentad2latlng(chk[\"PENTAD\"])\n",
    "\n",
    "    # Convert effort distance of the checklisst into degree lat-lon\n",
    "    effort_distance_lat = 180 / np.pi / 6371 * chk[\"EFFORT DISTANCE KM\"]\n",
    "    effort_distance_lon = (\n",
    "        180\n",
    "        / np.pi\n",
    "        / 6371\n",
    "        / np.cos(np.radians(chk[\"LATITUDE\"]))\n",
    "        * chk[\"EFFORT DISTANCE KM\"]\n",
    "    )\n",
    "\n",
    "    # Compute the distance from the center of the pentad (lat,lon) to the max distance possible if the observer traveled in the worst possible direction (i.e., to the closest eadge of the pentad)\n",
    "    # We relax a little bit the assumption of moving on the straight line to the edge of the pentad by applying a correction factor\n",
    "    corr_straight_line = 0.8\n",
    "    dist_lat = np.abs(lat - chk[\"LATITUDE\"]) + effort_distance_lat * corr_straight_line\n",
    "    dist_lon = np.abs(lon - chk[\"LONGITUDE\"]) + effort_distance_lon * corr_straight_line\n",
    "\n",
    "    # The maximum distance allowed for the checklist to be considered valid is half of a pentad resolution (5/60°)\n",
    "    # We accept that the checklist might have traveled a bit more that this distance\n",
    "    corr_overlap = 1.2  # allow for a 20% overlap\n",
    "    max_dist = (5 / 60 / 2) * corr_overlap\n",
    "\n",
    "    chk[\"KEEP PENTAD\"] = (dist_lat < max_dist) & (dist_lon < max_dist)\n",
    "\n",
    "    # Filter historical checklists which have no distance\n",
    "    chk.loc[\n",
    "        (chk[\"PROTOCOL TYPE\"] == \"Historical\") & chk[\"EFFORT DISTANCE KM\"].isna(),\n",
    "        \"KEEP PENTAD\",\n",
    "    ] = False\n",
    "\n",
    "    # Filter historical checklists which have no distance\n",
    "    chk.loc[\n",
    "        chk[\"EFFORT DISTANCE KM\"].isna(),\n",
    "        \"KEEP PENTAD\",\n",
    "    ] = True\n",
    "\n",
    "    chk.loc[\n",
    "        (chk[\"PROTOCOL TYPE\"] == \"Historical\") & chk[\"EFFORT DISTANCE KM\"].isna(),\n",
    "        \"KEEP PENTAD\",\n",
    "    ] = False\n",
    "\n",
    "    return chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = ebd2chk(ebd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all possible valid card\n",
    "\n",
    "Cards are considered to be full protocol if the sum of durations of the underlying checklists exceed 2 hours over the next rolling 5 days.\n",
    "In this section, we first indentify which checklists can create a valid full card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkday_pentad_observer(df):\n",
    "    # To find all pentad with sufficient duration effort (i.e, a sum of 2h over 5 days period), we apply this function for each pentad_observer.\n",
    "\n",
    "    df[\"CARD\"] = \"\"\n",
    "    # Build a matrix of distance between all checklists to check if they are close to each other\n",
    "    di = np.abs(\n",
    "        df[\"OBSERVATION DATE\"].values[:, None] - df[\"OBSERVATION DATE\"].values\n",
    "    ) < pd.Timedelta(days=5)\n",
    "    # create duration array to make computation slightly faster\n",
    "    duration = df[\"DURATION MINUTES\"].to_numpy()\n",
    "    # Initie the card array with empty string\n",
    "    # card = np.array(['' for x in range(len(df))], dtype='object')\n",
    "    u = 1\n",
    "    # Loop trough the list of checklists\n",
    "    while u <= len(df):\n",
    "        # Find all neighbord\n",
    "        nb_neighbor = np.sum(di[u - 1, (u - 1) :])\n",
    "        neigh = u + np.arange(0, nb_neighbor) - 1\n",
    "        dur = duration[neigh].sum()\n",
    "        # Check that total duration is more than 2hours, if so add card code (pentad_observer_date) to card array\n",
    "        if dur >= (2 * 60):\n",
    "            df.iloc[neigh, df.columns.get_loc(\"CARD\")] = df.iloc[\n",
    "                u - 1, df.columns.get_loc(\"pentad_observer_date\")\n",
    "            ]\n",
    "        u += nb_neighbor\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk2valid_card(chk):\n",
    "    # Find all possible valid card\n",
    "    # Cards are considered to be full protocol if the sum of durations of the underlying checklists exceed 2 hours over the next rolling 5 days.\n",
    "    # In this section, we first indentify which checklists can create a valid full card.\n",
    "\n",
    "    # Find the index of all checklists which contribute to the 2hr rule. Note that we will still use \"non-valid\" checklists later as their species still contribute to the card.\n",
    "    valid_id = (\n",
    "        chk[\"KEEP PENTAD\"]\n",
    "        & chk[\"KEEP PROTOCOL\"]\n",
    "        & (chk[\"DURATION MINUTES\"] > 0)\n",
    "        & chk[\"ALL SPECIES REPORTED\"]\n",
    "    )\n",
    "\n",
    "    # Filter for valid checklist and create in a smaller table\n",
    "    check = chk.loc[\n",
    "        valid_id, [\"PENTAD\", \"OBSERVER ID\", \"OBSERVATION DATE\", \"DURATION MINUTES\"]\n",
    "    ]\n",
    "\n",
    "    # Combine checklists made by the same observer, pentad, and day. This is an intermediate step which enables us to grid the 5 days windows more easily\n",
    "    checkday = (\n",
    "        check.groupby([\"PENTAD\", \"OBSERVER ID\", \"OBSERVATION DATE\"])\n",
    "        .agg({\"DURATION MINUTES\": \"sum\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort the checklist by date\n",
    "    checkday.sort_values(by=[\"OBSERVATION DATE\"], inplace=True)\n",
    "\n",
    "    # Create additional columns\n",
    "    checkday[\"pentad_observer\"] = checkday[\"PENTAD\"] + \"_\" + checkday[\"OBSERVER ID\"]\n",
    "    checkday[\"pentad_observer_date\"] = (\n",
    "        checkday[\"PENTAD\"]\n",
    "        + \"_\"\n",
    "        + checkday[\"OBSERVER ID\"].str[3:]\n",
    "        + \"_\"\n",
    "        + checkday[\"OBSERVATION DATE\"].dt.strftime(\"%Y%m%d\")\n",
    "    )\n",
    "\n",
    "    # Do a first filter to eliminate all pentad_observer witout sufficient total duration time. (Aim is to just reduce the computation later)\n",
    "    pentad_observer_duration = checkday.groupby([\"pentad_observer\"])[\n",
    "        \"DURATION MINUTES\"\n",
    "    ].sum()\n",
    "    pentad_observer_duration_index = pentad_observer_duration[\n",
    "        pentad_observer_duration >= 2 * 60\n",
    "    ].index\n",
    "    checkday_long = checkday[\n",
    "        checkday[\"pentad_observer\"].isin(pentad_observer_duration_index)\n",
    "    ]\n",
    "\n",
    "    # Second filter for reducing the test case\n",
    "    # pentad_observer_unique = checkday_long[\"pentad_observer\"].unique()\n",
    "    # pentad_observer_unique = pentad_observer_unique[0:1000]\n",
    "    # checkday_long = checkday_long[checkday_long[\"pentad_observer\"].isin(pentad_observer_unique)]\n",
    "\n",
    "    # Apply the function defined above for each pentad-observer at the same time (makes operation much faster)\n",
    "    checkday_long_card = (\n",
    "        checkday_long.groupby(\"pentad_observer\")\n",
    "        .apply(checkday_pentad_observer, include_groups=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create the DataFrame of all valid card\n",
    "    card_valid = checkday_long_card[\n",
    "        checkday_long_card[\"CARD\"] == checkday_long_card[\"pentad_observer_date\"]\n",
    "    ][[\"PENTAD\", \"OBSERVER ID\", \"OBSERVATION DATE\", \"CARD\"]]\n",
    "\n",
    "    # Sort by card\n",
    "    card_valid.sort_values(by=\"CARD\", inplace=True)\n",
    "\n",
    "    return card_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_valid = chk2valid_card(chk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Card dataframe by aggregating all checklists\n",
    "\n",
    "We take back `chk` where all checklists (i.e., including the incidentals, stationary, etc...) and find if they contribute to an existing full card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_card2chk_card(chk, card_valid):\n",
    "    # Create Card dataframe by aggregating all checklists\n",
    "    # We take back `chk` where all checklists (i.e., including the incidentals, stationary, etc...) and find if they contribute to an existing full card.\n",
    "    # Filter for checklist to keep: within pentad and and pentad and observer present in the valid card list\n",
    "    chk_keep = chk[\n",
    "        (chk[\"KEEP PENTAD\"])\n",
    "        & (\n",
    "            (chk[\"PENTAD\"] + chk[\"OBSERVER ID\"]).isin(\n",
    "                (card_valid[\"PENTAD\"] + card_valid[\"OBSERVER ID\"])\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Combine all possible checklits with the valid card based on observer and pentad.\n",
    "    # This will create duplicate checklist with all cards submitted by the same observer, same pentad, but any date\n",
    "    chk_card = pd.merge(\n",
    "        chk_keep,  # .loc[:,['OBSERVER ID', 'PENTAD', \"SAMPLING EVENT IDENTIFIER\", \"OBSERVATION DATE\"]],\n",
    "        card_valid,\n",
    "        on=[\"OBSERVER ID\", \"PENTAD\"],\n",
    "        suffixes=(\"_chk\", \"_card\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Filter the checklist for checklist beeing within the 5 days of the card so that there will be a single checklist-card now\n",
    "    duration = (\n",
    "        chk_card[\"OBSERVATION DATE_chk\"] - chk_card[\"OBSERVATION DATE_card\"]\n",
    "    ).dt.days\n",
    "    chk_card = chk_card[(duration >= 0) & (duration < 5)]\n",
    "\n",
    "    return chk_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_card = valid_card2chk_card(chk, card_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build card_chk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_card2card_chk(chk_card, card_valid):\n",
    "\n",
    "    # Cretate the card list with all checklists that belong to it. Compute aggregated value of all checklists\n",
    "    card_chk = (\n",
    "        chk_card.groupby(\"CARD\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"SAMPLING EVENT IDENTIFIER\": list,\n",
    "                \"OBSERVATION DATETIME\": [\"min\", \"max\"],\n",
    "                \"DURATION MINUTES\": \"sum\",\n",
    "                \"EFFORT DISTANCE KM\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    card_chk.columns = [\"_\".join(col).strip(\"_\") for col in card_chk.columns.values]\n",
    "\n",
    "    # merge with the information contained in card_valid\n",
    "    card_chk = pd.merge(card_chk, card_valid, on=\"CARD\", how=\"inner\")\n",
    "\n",
    "    return card_chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_chk = chk_card2card_chk(chk_card, card_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve species information per card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_card2ebd_f_u(ebd, chk_card):\n",
    "    # Filter the full dataset to get only the checklist used in the card data\n",
    "    ebd_f = ebd.loc[\n",
    "        ebd[\"SAMPLING EVENT IDENTIFIER\"].isin(chk_card[\"SAMPLING EVENT IDENTIFIER\"]),\n",
    "        [\n",
    "            \"SAMPLING EVENT IDENTIFIER\",\n",
    "            \"SCIENTIFIC NAME\",\n",
    "            \"TAXON CONCEPT ID\",\n",
    "            \"ADU\",\n",
    "            \"OBSERVATION DATETIME\",\n",
    "            \"LATITUDE\",\n",
    "            \"LONGITUDE\",\n",
    "            \"EFFORT DISTANCE KM\",\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Add card_id\n",
    "    ebd_f = pd.merge(\n",
    "        ebd_f,\n",
    "        chk_card.loc[:, [\"SAMPLING EVENT IDENTIFIER\", \"CARD\"]],\n",
    "        on=\"SAMPLING EVENT IDENTIFIER\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Keep a unique list of card-species (remove duplicate species in the same card, keeping the first one in time)\n",
    "    ebd_f.sort_values(\n",
    "        by=\"OBSERVATION DATETIME\", inplace=True\n",
    "    )  # SHould have been done already above, but necessary for keep=\"first\"\n",
    "\n",
    "    ebd_f_u = ebd_f.drop_duplicates(\n",
    "        subset=[\"CARD\", \"TAXON CONCEPT ID\"], keep=\"first\"\n",
    "    ).copy()\n",
    "    ebd_f_u.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Compute the sequence of records based on datetime entry\n",
    "    # ebd_f_u[\"SEQ\"] = (\n",
    "    #    ebd_f_u.groupby(\"CARD\")[\"OBSERVATION DATETIME\"].rank(method=\"min\").astype(int)\n",
    "    # )\n",
    "\n",
    "    # Compute the sequence basd on taxonomical order\n",
    "    ebd_f_u[\"SEQ\"] = (\n",
    "        ebd_f_u.groupby(\"CARD\")[\"TAXON CONCEPT ID\"]\n",
    "        .rank(method=\"min\")\n",
    "        .fillna(-1)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Not sure why, but fillina NA by nothing\n",
    "    ebd_f_u[\"EFFORT DISTANCE KM\"] = ebd_f_u[\"EFFORT DISTANCE KM\"].fillna(\"\")\n",
    "\n",
    "    # Convert datetime to standard format\n",
    "    ebd_f_u[\"OBSERVATION DATETIME\"] = ebd_f_u[\"OBSERVATION DATETIME\"].dt.strftime(\n",
    "        \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    )\n",
    "\n",
    "    return ebd_f_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd_f_u = chk_card2ebd_f_u(ebd, chk_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TAXON CONCEPT ID</th>\n",
       "      <th>SCIENTIFIC NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>OBSERVATION DATE</th>\n",
       "      <th>TIME OBSERVATIONS STARTED</th>\n",
       "      <th>OBSERVER ID</th>\n",
       "      <th>SAMPLING EVENT IDENTIFIER</th>\n",
       "      <th>PROTOCOL TYPE</th>\n",
       "      <th>DURATION MINUTES</th>\n",
       "      <th>EFFORT DISTANCE KM</th>\n",
       "      <th>ALL SPECIES REPORTED</th>\n",
       "      <th>OBSERVATION DATETIME</th>\n",
       "      <th>ADU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-7EE005BD</td>\n",
       "      <td>Phaethon aethereus</td>\n",
       "      <td>16.012736</td>\n",
       "      <td>-23.852151</td>\n",
       "      <td>1832-01-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obsr939641</td>\n",
       "      <td>S126285649</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1832-01-23 00:00:00</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-9B183BDD</td>\n",
       "      <td>Spheniscus demersus</td>\n",
       "      <td>-24.639209</td>\n",
       "      <td>14.530987</td>\n",
       "      <td>1845-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obsr4458114</td>\n",
       "      <td>S151363283</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1845-10-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-775E93D7</td>\n",
       "      <td>Morus capensis</td>\n",
       "      <td>-24.639209</td>\n",
       "      <td>14.530987</td>\n",
       "      <td>1845-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obsr4458114</td>\n",
       "      <td>S151363283</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1845-10-01 00:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-AAC2D613</td>\n",
       "      <td>Daption capense</td>\n",
       "      <td>-24.639209</td>\n",
       "      <td>14.530987</td>\n",
       "      <td>1845-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obsr4458114</td>\n",
       "      <td>S151363283</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1845-10-01 00:00:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-E6536E4E</td>\n",
       "      <td>Larus dominicanus</td>\n",
       "      <td>-24.639209</td>\n",
       "      <td>14.530987</td>\n",
       "      <td>1845-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obsr4458114</td>\n",
       "      <td>S151363283</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1845-10-01 00:00:00</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23474048</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-87EEBD3E</td>\n",
       "      <td>Botaurus sturmii</td>\n",
       "      <td>-18.456258</td>\n",
       "      <td>32.801380</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>22:10:00</td>\n",
       "      <td>obsr1691144</td>\n",
       "      <td>S207364222</td>\n",
       "      <td>Traveling</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-31 22:10:00</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23474049</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-87EEBD3E</td>\n",
       "      <td>Botaurus sturmii</td>\n",
       "      <td>-18.457479</td>\n",
       "      <td>32.801678</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>22:10:00</td>\n",
       "      <td>obsr1832653</td>\n",
       "      <td>S207364054</td>\n",
       "      <td>Traveling</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-31 22:10:00</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23474050</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-1EAFAD9F</td>\n",
       "      <td>Anas sparsa</td>\n",
       "      <td>-18.456258</td>\n",
       "      <td>32.801380</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>22:10:00</td>\n",
       "      <td>obsr1691144</td>\n",
       "      <td>S207364222</td>\n",
       "      <td>Traveling</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-31 22:10:00</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23474051</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-2E6575A8</td>\n",
       "      <td>Strix woodfordii</td>\n",
       "      <td>-29.476217</td>\n",
       "      <td>30.066271</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>22:28:00</td>\n",
       "      <td>obsr1903621</td>\n",
       "      <td>S207366390</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 22:28:00</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23474052</th>\n",
       "      <td>species</td>\n",
       "      <td>avibase-DA2F24E3</td>\n",
       "      <td>Alopochen aegyptiaca</td>\n",
       "      <td>18.111689</td>\n",
       "      <td>-15.966555</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>23:31:00</td>\n",
       "      <td>obsr3540712</td>\n",
       "      <td>S207389357</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-31 23:31:00</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23474053 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CATEGORY  TAXON CONCEPT ID       SCIENTIFIC NAME   LATITUDE  \\\n",
       "0         species  avibase-7EE005BD    Phaethon aethereus  16.012736   \n",
       "1         species  avibase-9B183BDD   Spheniscus demersus -24.639209   \n",
       "2         species  avibase-775E93D7        Morus capensis -24.639209   \n",
       "3         species  avibase-AAC2D613       Daption capense -24.639209   \n",
       "4         species  avibase-E6536E4E     Larus dominicanus -24.639209   \n",
       "...           ...               ...                   ...        ...   \n",
       "23474048  species  avibase-87EEBD3E      Botaurus sturmii -18.456258   \n",
       "23474049  species  avibase-87EEBD3E      Botaurus sturmii -18.457479   \n",
       "23474050  species  avibase-1EAFAD9F           Anas sparsa -18.456258   \n",
       "23474051  species  avibase-2E6575A8      Strix woodfordii -29.476217   \n",
       "23474052  species  avibase-DA2F24E3  Alopochen aegyptiaca  18.111689   \n",
       "\n",
       "          LONGITUDE OBSERVATION DATE TIME OBSERVATIONS STARTED  OBSERVER ID  \\\n",
       "0        -23.852151       1832-01-23                       NaN   obsr939641   \n",
       "1         14.530987       1845-10-01                       NaN  obsr4458114   \n",
       "2         14.530987       1845-10-01                       NaN  obsr4458114   \n",
       "3         14.530987       1845-10-01                       NaN  obsr4458114   \n",
       "4         14.530987       1845-10-01                       NaN  obsr4458114   \n",
       "...             ...              ...                       ...          ...   \n",
       "23474048  32.801380       2024-12-31                  22:10:00  obsr1691144   \n",
       "23474049  32.801678       2024-12-31                  22:10:00  obsr1832653   \n",
       "23474050  32.801380       2024-12-31                  22:10:00  obsr1691144   \n",
       "23474051  30.066271       2024-12-31                  22:28:00  obsr1903621   \n",
       "23474052 -15.966555       2024-12-31                  23:31:00  obsr3540712   \n",
       "\n",
       "         SAMPLING EVENT IDENTIFIER PROTOCOL TYPE  DURATION MINUTES  \\\n",
       "0                       S126285649    Historical               NaN   \n",
       "1                       S151363283    Historical               NaN   \n",
       "2                       S151363283    Historical               NaN   \n",
       "3                       S151363283    Historical               NaN   \n",
       "4                       S151363283    Historical               NaN   \n",
       "...                            ...           ...               ...   \n",
       "23474048                S207364222     Traveling              25.0   \n",
       "23474049                S207364054     Traveling              90.0   \n",
       "23474050                S207364222     Traveling              25.0   \n",
       "23474051                S207366390    Incidental               NaN   \n",
       "23474052                S207389357    Incidental               NaN   \n",
       "\n",
       "          EFFORT DISTANCE KM  ALL SPECIES REPORTED OBSERVATION DATETIME  ADU  \n",
       "0                        NaN                     0  1832-01-23 00:00:00  953  \n",
       "1                        NaN                     0  1845-10-01 00:00:00    2  \n",
       "2                        NaN                     0  1845-10-01 00:00:00   44  \n",
       "3                        NaN                     0  1845-10-01 00:00:00   14  \n",
       "4                        NaN                     0  1845-10-01 00:00:00  287  \n",
       "...                      ...                   ...                  ...  ...  \n",
       "23474048                 0.5                     1  2024-12-31 22:10:00   66  \n",
       "23474049                 3.0                     1  2024-12-31 22:10:00   66  \n",
       "23474050                 0.5                     1  2024-12-31 22:10:00   95  \n",
       "23474051                 NaN                     0  2024-12-31 22:28:00  362  \n",
       "23474052                 NaN                     0  2024-12-31 23:31:00   89  \n",
       "\n",
       "[23474053 rows x 15 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebd_f_u2card_exp(card_chk, ebd_f_u):\n",
    "\n",
    "    # Extract the species list per card as a cell for vectorized computation\n",
    "    card_sp = (\n",
    "        ebd_f_u.groupby(\"CARD\")[\n",
    "            [\n",
    "                \"TAXON CONCEPT ID\",\n",
    "                \"ADU\",\n",
    "                \"SEQ\",\n",
    "                \"LATITUDE\",\n",
    "                \"LONGITUDE\",\n",
    "                \"OBSERVATION DATETIME\",\n",
    "                \"EFFORT DISTANCE KM\",\n",
    "            ]\n",
    "        ]\n",
    "        .agg(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge the card information by checklist and species into an export card dataframe\n",
    "    card_exp = pd.merge(\n",
    "        card_chk,\n",
    "        card_sp,\n",
    "        on=\"CARD\",\n",
    "    )\n",
    "\n",
    "    # Set some default values\n",
    "    card_exp[\"Protocol\"] = \"F\"\n",
    "    card_exp[\"ObserverEmail\"] = \"kenyabirdmap@naturekenya.org\"\n",
    "    card_exp[\"ObserverNo\"] = \"22829\"\n",
    "\n",
    "    card_exp[\"Hour1\"] = \"\"\n",
    "    card_exp[\"Hour2\"] = \"\"\n",
    "    card_exp[\"Hour3\"] = \"\"\n",
    "    card_exp[\"Hour4\"] = \"\"\n",
    "    card_exp[\"Hour5\"] = \"\"\n",
    "    card_exp[\"Hour6\"] = \"\"\n",
    "    card_exp[\"Hour7\"] = \"\"\n",
    "    card_exp[\"Hour8\"] = \"\"\n",
    "    card_exp[\"Hour9\"] = \"\"\n",
    "    card_exp[\"Hour10\"] = \"\"\n",
    "    card_exp[\"InclNight\"] = \"0\"\n",
    "    card_exp[\"AllHabitats\"] = \"0\"\n",
    "\n",
    "    card_exp[\"TotalHours\"] = round(card_exp[\"DURATION MINUTES_sum\"] / 60, 2)\n",
    "    card_exp[\"TotalDistance\"] = round(card_exp[\"EFFORT DISTANCE KM_sum\"], 2)\n",
    "    card_exp[\"TotalSpp\"] = card_exp[\"ADU\"].apply(lambda x: len(x))\n",
    "    card_exp[\"StartDate\"] = card_exp[\"OBSERVATION DATETIME_min\"].dt.date.apply(str)\n",
    "    card_exp[\"EndDate\"] = card_exp[\"OBSERVATION DATETIME_max\"].dt.date.apply(str)\n",
    "    card_exp[\"StartTime\"] = card_exp[\"OBSERVATION DATETIME_min\"].dt.strftime(\"%H:%M\")\n",
    "\n",
    "    # Function that generate species record to be used for each species of each card\n",
    "    def create_records(\n",
    "        TAXON_CONCEPT_ID,\n",
    "        ADU,\n",
    "        SEQ,\n",
    "        LATITUDE,\n",
    "        LONGITUDE,\n",
    "        OBSERVATION_DATETIME,\n",
    "        EFFORT_DISTANCE_KM,\n",
    "        CARD,\n",
    "    ):\n",
    "        return [\n",
    "            {\n",
    "                \"Sequence\": SEQ,\n",
    "                \"Latitude\": LATITUDE,\n",
    "                \"Longitude\": LONGITUDE,\n",
    "                \"Altitude\": \"\",\n",
    "                \"CardNo\": CARD,\n",
    "                \"Spp\": ADU,\n",
    "                \"SourceSpp\": TAXON_CONCEPT_ID,\n",
    "                \"Accuracy\": EFFORT_DISTANCE_KM * 1000,\n",
    "                \"SightingTime\": OBSERVATION_DATETIME,\n",
    "            }\n",
    "            for TAXON_CONCEPT_ID, ADU, SEQ, LATITUDE, LONGITUDE, OBSERVATION_DATETIME, EFFORT_DISTANCE_KM in zip(\n",
    "                TAXON_CONCEPT_ID,\n",
    "                ADU,\n",
    "                SEQ,\n",
    "                LATITUDE,\n",
    "                LONGITUDE,\n",
    "                OBSERVATION_DATETIME,\n",
    "                EFFORT_DISTANCE_KM,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Apply the function\n",
    "    card_exp[\"records\"] = card_exp.apply(\n",
    "        lambda row: create_records(\n",
    "            row[\"TAXON CONCEPT ID\"],\n",
    "            row[\"ADU\"],\n",
    "            row[\"SEQ\"],\n",
    "            row[\"LATITUDE\"],\n",
    "            row[\"LONGITUDE\"],\n",
    "            row[\"OBSERVATION DATETIME\"],\n",
    "            row[\"EFFORT DISTANCE KM\"],\n",
    "            row[\"CARD\"],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Rename to match ABAP server input\n",
    "    card_exp = card_exp.rename(\n",
    "        columns={\n",
    "            \"CARD\": \"CardNo\",\n",
    "            \"PENTAD\": \"Pentad\",\n",
    "            \"SAMPLING EVENT IDENTIFIER\": \"Checklists\",\n",
    "            \"OBSERVER ID\": \"ObserverNoEbird\",\n",
    "            \"SAMPLING EVENT IDENTIFIER_list\": \"Checklists\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    card_exp = card_exp.reindex(\n",
    "        columns=[\n",
    "            \"Protocol\",\n",
    "            \"ObserverEmail\",\n",
    "            \"CardNo\",\n",
    "            \"StartDate\",\n",
    "            \"EndDate\",\n",
    "            \"StartTime\",\n",
    "            \"Pentad\",\n",
    "            \"ObserverNo\",\n",
    "            \"TotalHours\",\n",
    "            \"Hour1\",\n",
    "            \"Hour2\",\n",
    "            \"Hour3\",\n",
    "            \"Hour4\",\n",
    "            \"Hour5\",\n",
    "            \"Hour6\",\n",
    "            \"Hour7\",\n",
    "            \"Hour8\",\n",
    "            \"Hour9\",\n",
    "            \"Hour10\",\n",
    "            \"TotalSpp\",\n",
    "            \"InclNight\",\n",
    "            \"AllHabitats\",\n",
    "            \"Checklists\",\n",
    "            \"TotalDistance\",\n",
    "            \"ObserverNoEbird\",\n",
    "            \"records\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return card_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_exp = ebd_f_u2card_exp(card_chk, ebd_f_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Sequence': 17,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 385,\n",
       "  'SourceSpp': 'avibase-D209A90C',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 2,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 594,\n",
       "  'SourceSpp': 'avibase-14AFBA82',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 11,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 0,\n",
       "  'SourceSpp': 'avibase-6ABDB635',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 5,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 1152,\n",
       "  'SourceSpp': 'avibase-20D915AD',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 16,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 576,\n",
       "  'SourceSpp': 'avibase-CF2E9674',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 3,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 510,\n",
       "  'SourceSpp': 'avibase-1AED683B',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 15,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 629,\n",
       "  'SourceSpp': 'avibase-AEE2063F',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 12,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 499,\n",
       "  'SourceSpp': 'avibase-97E7A30B',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 18,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 3852,\n",
       "  'SourceSpp': 'avibase-E10D0809',\n",
       "  'Accuracy': 800.0,\n",
       "  'SightingTime': '2022-08-10T11:55:00Z'},\n",
       " {'Sequence': 8,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 3644,\n",
       "  'SourceSpp': 'avibase-3D3EF8AC',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 7,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 1572,\n",
       "  'SourceSpp': 'avibase-32E6DB03',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 6,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 3608,\n",
       "  'SourceSpp': 'avibase-23BEFB19',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 1,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 112,\n",
       "  'SourceSpp': 'avibase-14885C18',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 19,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 3040,\n",
       "  'SourceSpp': 'avibase-EBE56736',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 10,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 503,\n",
       "  'SourceSpp': 'avibase-5ACC908D',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 9,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 1492,\n",
       "  'SourceSpp': 'avibase-4B8F7AF5',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 13,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 2104,\n",
       "  'SourceSpp': 'avibase-A16C3B99',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 4,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 3700,\n",
       "  'SourceSpp': 'avibase-1DD0949C',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'},\n",
       " {'Sequence': 14,\n",
       "  'Latitude': -0.0392179,\n",
       "  'Longitude': 9.340437,\n",
       "  'Altitude': '',\n",
       "  'CardNo': '0000_0920_r431479_20220810',\n",
       "  'Spp': 1584,\n",
       "  'SourceSpp': 'avibase-A90C512F',\n",
       "  'Accuracy': 1800.0,\n",
       "  'SightingTime': '2022-08-10T13:56:00Z'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_exp[\"records\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = card_exp.to_json(orient=\"records\", indent=2)\n",
    "with open(\n",
    "    f\"../export/ebd_AFR_rel{month}-{year}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export in better format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_chk.to_csv(\n",
    "    f\"../export/ebd_AFR_rel{month}-{year}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_cards.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd_f_u[[\"CARD\", \"ADU\", \"SEQ\"]].to_csv(\n",
    "    f\"../export/ebd_AFR_rel{month}-{year}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_records.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
